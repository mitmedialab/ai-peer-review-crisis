{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import functions.prompts as prompts\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=config['DEEPINFRA_TOKEN'],\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dump/csv/fake_papers.csv\")\n",
    "# df.iloc[np.r_[0:4, -4:0]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_body(text, top5=True, model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"):\n",
    "    return {\n",
    "            \"model\": model,\n",
    "            \"messages\":[{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"{prompts.top5() if top5 else prompts.analysis()}\\nPlease respond in valid JSON format that matches this schema: {str(prompts.Top5Model.model_json_schema() if top5 else prompts.AnalysisModel.model_json_schema())}. **IMPORTANT**: ONLY RESPOND WITH AN JSON OBJECT CONTAINING SCORES ACCORDING TO THE ABOVE SCHEMA. THE RESPONSE MUST END WITH A CURLY BRACKET. DO NOT ADD ANALYSIS OR EXPLANATION.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text\n",
    "                }, \n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"{\"\n",
    "                }],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_paper(client, i, df):\n",
    "    text = \"\"\n",
    "    f = open(f\"output-fake/{df.iloc[i]['id']}.txt\", \"r\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # paper = f\"PAPER TITLE: {df.iloc[i]['name']}\\n\\nPAPER TEXT: {text}\"\n",
    "    paper = text\n",
    "    # gen_body(paper)\n",
    "    \n",
    "    # model = \"google/gemma-3-27b-it\"\n",
    "    model = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "    \n",
    "    scores = [ client.chat.completions.create(**gen_body(paper, model=model)) for _ in range(3)]\n",
    "    analysis = [ client.chat.completions.create(**gen_body(paper, top5=False, model=model)) for _ in range(3)]\n",
    "    \n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"analysis\": analysis\n",
    "    }\n",
    "    \n",
    "def parse_r(r, id, typ):\n",
    "    # id = r.custom_id\n",
    "    validateModel = prompts.Top5Model if typ == \"top5\" else prompts.AnalysisModel\n",
    "    try:\n",
    "        # print(r)\n",
    "        text = r.choices[0].message.content\n",
    "        if(text.startswith(\"```json\")):\n",
    "            text = text.split(\"```json\")[1].split(\"}\")[0].replace(\"'\", '\"') + \"}\"\n",
    "        else:\n",
    "            text = \"{\" + text.split(\"}\")[0].replace(\"'\", '\"') + \"}\"\n",
    "        if(text.startswith(\"{{\")):\n",
    "            text = text[1:]\n",
    "        return {\n",
    "            \"id\": id,\n",
    "            \"scores\": validateModel.model_validate(json.loads(text)).model_dump()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e} - {\"{\" + r.choices[0].message.content}\")\n",
    "        return {\n",
    "            \"id\": id,\n",
    "            \"scores\": None\n",
    "        }\n",
    "\n",
    "def parse_paper(rs):\n",
    "    scores = [ parse_r(x, j, \"top5\") for j, x in enumerate(rs['scores']) ]\n",
    "    analysis = [ parse_r(x, j, \"analysis\") for j, x in enumerate(rs['analysis']) ]\n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"analysis\": analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama\"\n",
    "\n",
    "def update_df(df, i, no, score, typ):\n",
    "    metrics = ['score'] if typ == \"top5\" else ['originality', 'rigor', 'scope', 'impact', 'written_by_ai']\n",
    "    validateModel = prompts.Top5Model if typ == \"top5\" else prompts.AnalysisModel\n",
    "\n",
    "    for _, metric in enumerate(metrics):\n",
    "        column_name = f\"{model_name}-{metric}-{int(no)+1}\"\n",
    "        \n",
    "        if column_name not in df.columns:\n",
    "            df[column_name] = None\n",
    "\n",
    "        try:\n",
    "            o = validateModel.model_validate(score)\n",
    "            df.loc[i, column_name] = o.__dict__[metric]\n",
    "        except:\n",
    "            print(f\"ERROR | Can't update the model in in {column_name}, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_paper(client, i, df):\n",
    "    print(f\"Evaluating {i} -> {df.iloc[i]['name']}\")\n",
    "    \n",
    "    x = llm_paper(client, i, df)\n",
    "    y = parse_paper(x)\n",
    "    \n",
    "    for j, s in enumerate(y['scores']):\n",
    "        update_df(df, i, j, s['scores'], \"top5\")\n",
    "    \n",
    "    for j, a in enumerate(y['analysis']):\n",
    "        update_df(df, i, j, a['scores'], \"analysis\")\n",
    "    # update_df(df, i, 0, y['scores'][0]['scores'], \"top5\")\n",
    "    # idx = df.index[df['id'] == id].tolist()[0]\n",
    "    # content = l['response']['body']['choices'][0]['message']['content']\n",
    "    \n",
    "# evaluate_paper(client, 0, df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "chunk = 30\n",
    "for i in range(0, len(df), chunk):\n",
    "    print(f\"PROCESSING CHUNK {(i // chunk) + 1} of {math.ceil(len(df) / chunk)}\")\n",
    "    with ThreadPoolExecutor(max_workers=200) as executor:\n",
    "        results = list(executor.map(\n",
    "            evaluate_paper, \n",
    "            [client] * chunk,\n",
    "            [j for j in range(i, min(i+chunk, len(df)))],\n",
    "            [df] * len(df[\"file\"]),\n",
    "        ))\n",
    "        \n",
    "    df.to_csv(f\"{model_name}/{(i // chunk) + 1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fallback\n",
    "# fallback = df[df.isna().any(axis=1)].index\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=200) as executor:\n",
    "#         results = list(executor.map(\n",
    "#             evaluate_paper, \n",
    "#             [client] * len(fallback),\n",
    "#             fallback,\n",
    "#             [df] * len(fallback),\n",
    "#         ))\n",
    "        \n",
    "# df.to_csv(f\"{model_name}/fallback.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"fake_l.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
