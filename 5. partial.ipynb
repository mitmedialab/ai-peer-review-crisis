{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import anthropic\n",
        "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
        "from anthropic.types.messages.batch_create_params import Request\n",
        "\n",
        "import json\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "import functions.prompts as prompts\n",
        "\n",
        "client = anthropic.Anthropic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"dump/csv/papers.csv\")\n",
        "df['rank'] = df['id'].apply(lambda x: x.split(\"_\")[0])\n",
        "df = df.loc[df['rank'].isin([\"1\", \"25\", \"50\", \"75\", \"100\"])].reset_index(drop=True)\n",
        "\n",
        "len(df)\n",
        "\n",
        "# random -> only real paper / is not in 1/25/50/75/100 -> sample n=10\n",
        "# rand = df[:-100].loc[~df['rank'].isin([\"1\", \"25\", \"50\", \"75\", \"100\"])].sample(n=10, random_state=42)\n",
        "\n",
        "# sample = pd.concat([ranks, rand]).reset_index(drop=True)\n",
        "# sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def req(id, text, top5=True):\n",
        "    return Request(\n",
        "        custom_id=id,\n",
        "        params=MessageCreateParamsNonStreaming(\n",
        "            model=\"claude-3-5-haiku-20241022\",\n",
        "            max_tokens=1024,\n",
        "            system=f\"{prompts.top5() if top5 else prompts.analysis()}\\nPlease respond in valid JSON format that matches this schema: {str(prompts.Top5Model.model_json_schema() if top5 else prompts.AnalysisModel.model_json_schema())}. **IMPORTANT**: ONLY RESPOND WITH A JSON OBJECT CONTAINING SCORES ACCORDING TO THE ABOVE SCHEMA. THE RESPONSE MUST END WITH A CURLY BRACKET. DO NOT ADD ANALYSIS OR EXPLANATION.\",\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": text\n",
        "            }, {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"{\"\n",
        "            }]\n",
        "        )\n",
        "    )\n",
        "    \n",
        "def batch(text, id):\n",
        "    return [    *[ req(f\"{id}Z{i}Qtop5\", text) for i in range(3) ],\n",
        "                *[ req(f\"{id}Z{i}Qanalysis\", text, top5=False) for i in range(3) ] ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_req = {}\n",
        "divider = 1\n",
        "for i in range(divider):\n",
        "    full_req[f'batch-{i}'] = []\n",
        "\n",
        "def partial(paper, no):\n",
        "    l = len(paper.split())\n",
        "    # if(no == 0.01):\n",
        "        # print(\"asdf\", math.ceil((l * no) / 100))\n",
        "    return \" \".join(paper.split()[:math.ceil((l * no) / 100)])\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    file_index = math.floor(index / (len(df) / divider))\n",
        "    with open(f'output/{row[\"id\"]}.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "        paper = f\"PAPER TITLE: {row['name']}\\n\\nPAPER TEXT: {text}\"\n",
        "        \n",
        "        for no in [0.1, 1, 10, 50, 100]:\n",
        "            full_req[f\"batch-{file_index}\"] += batch(partial(paper, no), row[\"id\"]+\"P\"+str(no).split(\".\")[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(divider):\n",
        "    print(len(full_req[f'batch-{i}']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_req[f'batch-{i}'][10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KINDA DANGEROUS\n",
        "batches = []\n",
        "for i in range(divider):\n",
        "    print(f\"Sending Batch {i}\")\n",
        "    message_batch = client.messages.batches.create(\n",
        "        requests=full_req[f'batch-{i}'])\n",
        "    print(f\"{i} {message_batch.id}\")\n",
        "    batches.append(message_batch)\n",
        "\n",
        "print(batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "id = \"\"\n",
        "def wait(id):\n",
        "    results = client.messages.batches.retrieve(id).processing_status\n",
        "    while results == \"in_progress\":\n",
        "        stat = client.messages.batches.retrieve(id)\n",
        "        print(stat.request_counts)\n",
        "        results = stat.processing_status\n",
        "        time.sleep(5)\n",
        "\n",
        "wait(id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batches = []\n",
        "with open(\"dump/anthropic-batch/anthropic-batch-partial.txt\", 'r') as f:\n",
        "    batches = f.read()\n",
        "    batches = batches.split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_r(r):\n",
        "    id = r.custom_id\n",
        "    validateModel = prompts.Top5Model if \"top5\" in id else prompts.AnalysisModel\n",
        "    try:\n",
        "        text = r.result.message.content[0].text\n",
        "        text = \"{\" + text.split(\"}\")[0] + \"}\"\n",
        "        return {\n",
        "            \"id\": id,\n",
        "            \"scores\": validateModel.model_validate(json.loads(text)).model_dump()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error {e} - {\"{\" + r.result.message.content[0].text}\")\n",
        "        return {\n",
        "            \"id\": id,\n",
        "            \"scores\": None\n",
        "        }\n",
        "\n",
        "for b in batches:\n",
        "    results = client.messages.batches.results(b)\n",
        "    for r in results:\n",
        "        if(r and r.result.type == 'succeeded'):\n",
        "            try:\n",
        "                with open('dump/eval-output/anthropic-result-partial.jsonl', 'a') as f:\n",
        "                    f.write(json.dumps(parse_r(r)) + \"\\n\")\n",
        "            except Exception as e:\n",
        "                print(\"ERROR! - \" + str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"dump/csv/segments.csv\")\n",
        "\n",
        "f = open('dump/eval-output/anthropic-result-partial.jsonl', 'r')\n",
        "file_response = f.read()\n",
        "f.close()\n",
        "\n",
        "for line in file_response.split(\"\\n\")[:-1]:\n",
        "    l = json.loads(line)\n",
        "    id, s = l['id'].split(\"Z\")[0].split(\"P\")\n",
        "    no, typ = l['id'].split(\"Z\")[1].split(\"Q\")\n",
        "    \n",
        "    idx = df.index[df['id'] == id].tolist()[0]\n",
        "    \n",
        "    # content = l['response']['body']['choices'][0]['message']['content']\n",
        "    \n",
        "    metrics = ['score'] if typ == \"top5\" else ['originality', 'rigor', 'scope', 'impact', 'written_by_ai']\n",
        "    validateModel = prompts.Top5Model if typ == \"top5\" else prompts.AnalysisModel\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        column_name = f\"anthropic-{s}-{metric}-{int(no)+1}\"\n",
        "        \n",
        "        if column_name not in df.columns:\n",
        "            df[column_name] = None\n",
        "\n",
        "        o = validateModel.model_validate(l['scores'])\n",
        "        df.loc[idx, column_name] = o.__dict__[metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['rank'] = df['id'].apply(lambda x: x.split(\"_\")[0])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"dump/csv/result_partial.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
