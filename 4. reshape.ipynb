{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dump/csv/bias_oalg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "      <th>name</th>\n",
       "      <th>journal</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>len-original</th>\n",
       "      <th>len-anond</th>\n",
       "      <th>rank</th>\n",
       "      <th>anthropic-ins0-score-1</th>\n",
       "      <th>...</th>\n",
       "      <th>openai-ran0-score-1</th>\n",
       "      <th>openai-ran1-score-1</th>\n",
       "      <th>openai-ran2-score-1</th>\n",
       "      <th>openai-ran3-score-1</th>\n",
       "      <th>openai-ran4-score-1</th>\n",
       "      <th>openai-ran5-score-1</th>\n",
       "      <th>openai-ran6-score-1</th>\n",
       "      <th>openai-ran7-score-1</th>\n",
       "      <th>openai-ran8-score-1</th>\n",
       "      <th>openai-ran9-score-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100_0</td>\n",
       "      <td>100. Review of Quant Finance and Accounting/s1...</td>\n",
       "      <td>Does the presence of a sustainability committe...</td>\n",
       "      <td>Review of Quant Finance and Accounting</td>\n",
       "      <td>Supun Chandrasena; Lane Matthews; Ali Meftah G...</td>\n",
       "      <td>Supun Chandrasena1;  Queen’s Business School, ...</td>\n",
       "      <td>11053.0</td>\n",
       "      <td>10632.0</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               file  \\\n",
       "0  100_0  100. Review of Quant Finance and Accounting/s1...   \n",
       "\n",
       "                                                name  \\\n",
       "0  Does the presence of a sustainability committe...   \n",
       "\n",
       "                                  journal  \\\n",
       "0  Review of Quant Finance and Accounting   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Supun Chandrasena; Lane Matthews; Ali Meftah G...   \n",
       "\n",
       "                                        affiliations  len-original  len-anond  \\\n",
       "0  Supun Chandrasena1;  Queen’s Business School, ...       11053.0    10632.0   \n",
       "\n",
       "  rank  anthropic-ins0-score-1  ...  openai-ran0-score-1  openai-ran1-score-1  \\\n",
       "0  100                       5  ...                    4                    3   \n",
       "\n",
       "   openai-ran2-score-1  openai-ran3-score-1  openai-ran4-score-1  \\\n",
       "0                    4                    4                    4   \n",
       "\n",
       "   openai-ran5-score-1  openai-ran6-score-1  openai-ran7-score-1  \\\n",
       "0                    3                    4                    4   \n",
       "\n",
       "   openai-ran8-score-1  openai-ran9-score-1  \n",
       "0                    4                    3  \n",
       "\n",
       "[1 rows x 117 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = [col for col in df.columns if col[-1].isnumeric() and col not in [\"Unnamed: 0\", \"Unnamed: 0.1\", \"Unnamed: 0.2\", \"Unnamed: 0.3\"]]\n",
    "\n",
    "l_cols = [\"-\".join([*col.split(\"-\")[2:]]) for col in df_cols]\n",
    "cols = [\"-\".join([col.split(\"-\")[0], *col.split(\"-\")[2:]]) for col in df_cols]\n",
    "\n",
    "author = [col.split(\"-\")[1] for col in df_cols]\n",
    "author = [*set(author)]\n",
    "\n",
    "cols = [*set(cols)]\n",
    "models = [\"llama\", \"openai\", \"gemma\", \"anthropic\"]\n",
    "\n",
    "reshape = pd.DataFrame(columns=[*['uid', 'id'], *cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'file',\n",
       " 'name',\n",
       " 'journal',\n",
       " 'authors',\n",
       " 'affiliations',\n",
       " 'len-original',\n",
       " 'len-anond',\n",
       " 'rank',\n",
       " 'anthropic-ins0-score-1',\n",
       " 'anthropic-ins1-score-1',\n",
       " 'anthropic-ins2-score-1',\n",
       " 'anthropic-ins3-score-1',\n",
       " 'anthropic-ins4-score-1',\n",
       " 'anthropic-ins5-score-1',\n",
       " 'anthropic-top0-score-1',\n",
       " 'anthropic-top1-score-1',\n",
       " 'anthropic-top2-score-1',\n",
       " 'anthropic-top3-score-1',\n",
       " 'anthropic-top4-score-1',\n",
       " 'anthropic-top5-score-1',\n",
       " 'anthropic-top6-score-1',\n",
       " 'anthropic-top7-score-1',\n",
       " 'anthropic-top8-score-1',\n",
       " 'anthropic-top9-score-1',\n",
       " 'anthropic-ran0-score-1',\n",
       " 'anthropic-ran1-score-1',\n",
       " 'anthropic-ran2-score-1',\n",
       " 'anthropic-ran3-score-1',\n",
       " 'anthropic-ran4-score-1',\n",
       " 'anthropic-ran5-score-1',\n",
       " 'anthropic-ran6-score-1',\n",
       " 'anthropic-ran7-score-1',\n",
       " 'anthropic-ran8-score-1',\n",
       " 'anthropic-ran9-score-1',\n",
       " 'Unnamed: 0.3',\n",
       " 'Unnamed: 0.2',\n",
       " 'Unnamed: 0.1',\n",
       " 'Unnamed: 0',\n",
       " 'gemma-ins4-score-1',\n",
       " 'gemma-ins1-score-1',\n",
       " 'gemma-ran1-score-1',\n",
       " 'gemma-top7-score-1',\n",
       " 'gemma-ran3-score-1',\n",
       " 'gemma-top0-score-1',\n",
       " 'gemma-ins2-score-1',\n",
       " 'gemma-ins3-score-1',\n",
       " 'gemma-ins0-score-1',\n",
       " 'gemma-ran2-score-1',\n",
       " 'gemma-top1-score-1',\n",
       " 'gemma-top2-score-1',\n",
       " 'gemma-top5-score-1',\n",
       " 'gemma-top4-score-1',\n",
       " 'gemma-top9-score-1',\n",
       " 'gemma-ran0-score-1',\n",
       " 'gemma-top8-score-1',\n",
       " 'gemma-ran4-score-1',\n",
       " 'gemma-ran5-score-1',\n",
       " 'gemma-ran6-score-1',\n",
       " 'gemma-ran7-score-1',\n",
       " 'gemma-top6-score-1',\n",
       " 'gemma-ins5-score-1',\n",
       " 'gemma-top3-score-1',\n",
       " 'gemma-ran9-score-1',\n",
       " 'gemma-ran8-score-1',\n",
       " 'llama-ins4-score-1',\n",
       " 'llama-ins2-score-1',\n",
       " 'llama-top4-score-1',\n",
       " 'llama-top8-score-1',\n",
       " 'llama-ran0-score-1',\n",
       " 'llama-ins0-score-1',\n",
       " 'llama-ran7-score-1',\n",
       " 'llama-ins5-score-1',\n",
       " 'llama-ins1-score-1',\n",
       " 'llama-ran2-score-1',\n",
       " 'llama-top9-score-1',\n",
       " 'llama-top3-score-1',\n",
       " 'llama-top2-score-1',\n",
       " 'llama-top1-score-1',\n",
       " 'llama-ran4-score-1',\n",
       " 'llama-top5-score-1',\n",
       " 'llama-top7-score-1',\n",
       " 'llama-ran9-score-1',\n",
       " 'llama-top6-score-1',\n",
       " 'llama-top0-score-1',\n",
       " 'llama-ins3-score-1',\n",
       " 'llama-ran5-score-1',\n",
       " 'llama-ran1-score-1',\n",
       " 'llama-ran6-score-1',\n",
       " 'llama-ran3-score-1',\n",
       " 'llama-ran8-score-1',\n",
       " 'openai-ins0-score-1',\n",
       " 'openai-ins1-score-1',\n",
       " 'openai-ins2-score-1',\n",
       " 'openai-ins3-score-1',\n",
       " 'openai-ins4-score-1',\n",
       " 'openai-ins5-score-1',\n",
       " 'openai-top0-score-1',\n",
       " 'openai-top1-score-1',\n",
       " 'openai-top2-score-1',\n",
       " 'openai-top3-score-1',\n",
       " 'openai-top4-score-1',\n",
       " 'openai-top5-score-1',\n",
       " 'openai-top6-score-1',\n",
       " 'openai-top7-score-1',\n",
       " 'openai-top8-score-1',\n",
       " 'openai-top9-score-1',\n",
       " 'openai-ran0-score-1',\n",
       " 'openai-ran1-score-1',\n",
       " 'openai-ran2-score-1',\n",
       " 'openai-ran3-score-1',\n",
       " 'openai-ran4-score-1',\n",
       " 'openai-ran5-score-1',\n",
       " 'openai-ran6-score-1',\n",
       " 'openai-ran7-score-1',\n",
       " 'openai-ran8-score-1',\n",
       " 'openai-ran9-score-1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to transform the DataFrame\n",
    "def transform_dataframe(df):\n",
    "    # List of columns needed in the new DataFrame\n",
    "    base_cols = ['id', 'file', 'name', 'journal', 'authors', 'affiliations', 'len-original', 'len-anond', 'rank']\n",
    "    \n",
    "    # Regular expression to extract model, author type, metric, and annotator from column names\n",
    "    pattern = r'(anthropic|gemma|openai|llama)-(ins\\d+|top\\d+|ran\\d+)-(score|originality|rigor|scope|impact|written_by_ai)-(\\d+)'\n",
    "    \n",
    "    # Dictionary to store the transformed data\n",
    "    transformed_data = []\n",
    "    \n",
    "    # Get unique row identifiers (assuming 'id' is a unique identifier)\n",
    "    if 'id' in df.columns:\n",
    "        row_ids = df['id'].unique()\n",
    "    else:\n",
    "        # Create dummy row IDs if 'id' column doesn't exist\n",
    "        row_ids = range(len(df))\n",
    "    \n",
    "    # Process each row in the original DataFrame\n",
    "    for idx, row_id in enumerate(row_ids):\n",
    "        if 'id' in df.columns:\n",
    "            row = df[df['id'] == row_id].iloc[0]\n",
    "        else:\n",
    "            row = df.iloc[idx]\n",
    "        \n",
    "        # Store base columns\n",
    "        base_data = {}\n",
    "        for col in base_cols:\n",
    "            if col in df.columns:\n",
    "                base_data[col] = row[col]\n",
    "        \n",
    "        # Create a dictionary to identify unique author types in this row\n",
    "        author_types = {}\n",
    "        \n",
    "        # First pass - identify all the author types\n",
    "        for col in df.columns:\n",
    "            match = re.match(pattern, col)\n",
    "            if match:\n",
    "                model, author_type, metric, annotator = match.groups()\n",
    "                \n",
    "                # Create a unique key for this author type\n",
    "                author_key = author_type\n",
    "                \n",
    "                # Initialize dictionary for this author if not exists\n",
    "                if author_key not in author_types:\n",
    "                    author_types[author_key] = {\n",
    "                        'uid': idx,\n",
    "                        'id': base_data.get('id', idx),\n",
    "                        'author': author_type\n",
    "                    }\n",
    "                    # Add base data\n",
    "                    for base_col, value in base_data.items():\n",
    "                        author_types[author_key][base_col] = value\n",
    "                \n",
    "                # Add the metric with model prefix\n",
    "                metric_key = f\"{model}-{metric}-{annotator}\"\n",
    "                author_types[author_key][metric_key] = row[col]\n",
    "        \n",
    "        # Add each author record to the transformed data\n",
    "        transformed_data.extend(author_types.values())\n",
    "    \n",
    "    # Create the new DataFrame\n",
    "    new_df = pd.DataFrame(transformed_data)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = transform_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv(\"reshape.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
