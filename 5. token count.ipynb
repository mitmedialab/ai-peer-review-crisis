{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import functions.prompts as prompts\n",
    "import tiktoken\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "\n",
    "gemma = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.3-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "    num_tokens = 0\n",
    "\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(model, text):\n",
    "    if(model == \"llama\"):\n",
    "        token = llama(text, return_tensors=\"pt\")\n",
    "        return token['input_ids'].shape[1]\n",
    "    elif(model == \"gemma\"):\n",
    "        token = gemma(text, return_tensors=\"pt\")\n",
    "        return token['input_ids'].shape[1]\n",
    "    elif(model == 'openai'):\n",
    "        return num_tokens_from_messages([{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }])\n",
    "    else:\n",
    "        response = client.messages.count_tokens(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        system=\"\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "            }]\n",
    "        )\n",
    "        return response.input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dump/csv/papers.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "\n",
    "def process_row(index, row):\n",
    "    if index % 10 == 0:\n",
    "        print(index)\n",
    "    \n",
    "    try:\n",
    "        with open(f\"output/{row['id']}.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "            \n",
    "            results = {}\n",
    "            for m in [\"llama\", \"gemma\", \"openai\", \"anthropic\"]:\n",
    "                results[m] = token(m, text)\n",
    "            \n",
    "            return index, results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {index}: {e}\")\n",
    "        return index, None\n",
    "\n",
    "def main():\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=500) as executor:\n",
    "        # Submit all tasks to the executor\n",
    "        future_to_index = {executor.submit(process_row, index, row): index \n",
    "                          for index, row in df.iterrows()}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_index):\n",
    "            index, result = future.result()\n",
    "            if result:\n",
    "                results.append((index, result))\n",
    "    \n",
    "    # Update the dataframe with results\n",
    "    for index, result_dict in results:\n",
    "        if result_dict:\n",
    "            for m, value in result_dict.items():\n",
    "                df.loc[index, m] = value\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"token.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(text):\n",
    "    return [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": f\"{prompts.top5()}\\nPlease respond in valid JSON format that matches this schema: {str(prompts.Top5Model.model_json_schema())}. **IMPORTANT**: ONLY RESPOND WITH A JSON OBJECT CONTAINING SCORES ACCORDING TO THE ABOVE SCHEMA. THE RESPONSE MUST END WITH A CURLY BRACKET. DO NOT ADD ANALYSIS OR EXPLANATION.\"\n",
    "}, {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": text\n",
    "}, {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"{\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/1_0.txt\", \"r\")\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "response = client.messages.count_tokens(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    system=f\"{prompts.top5()}\\nPlease respond in valid JSON format that matches this schema: {str(prompts.Top5Model.model_json_schema())}. **IMPORTANT**: ONLY RESPOND WITH A JSON OBJECT CONTAINING SCORES ACCORDING TO THE ABOVE SCHEMA. THE RESPONSE MUST END WITH A CURLY BRACKET. DO NOT ADD ANALYSIS OR EXPLANATION.\",\n",
    "    messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": text\n",
    "}, {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"{\"\n",
    "}]\n",
    "    )\n",
    "\n",
    "print(response.input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openaiClient = OpenAI()\n",
    "\n",
    "x = openaiClient.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompts.top5()\n",
    "        }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    }],\n",
    "    response_format=prompts.Top5Model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {\n",
    "    \"anthropic\": 343,\n",
    "    \"gemma\": 315,\n",
    "    \"llama\": 324,\n",
    "    \"openai\": 248\n",
    "}\n",
    "\n",
    "s = {\n",
    "    \"anthropic\": 0,\n",
    "    \"gemma\": 0,\n",
    "    \"llama\": 0,\n",
    "    \"openai\": 0\n",
    "}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for m in [\"llama\", \"gemma\", \"openai\", \"anthropic\"]:\n",
    "        s[m] += row[m] + t[m]\n",
    "\n",
    "for m in [\"llama\", \"gemma\", \"openai\", \"anthropic\"]:\n",
    "    print(m, s[m]/df.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
