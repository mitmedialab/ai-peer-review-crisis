{"cells":[{"cell_type":"code","metadata":{"source_hash":"24057ba9","execution_start":1737364443360,"execution_millis":5181,"execution_context_id":"9db1e84b-05d6-44b8-9ca3-cf28f607da42","cell_id":"6587ee59087c4142a8371ad4c485663b","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport json\nfrom openai import OpenAI\nfrom datetime import datetime\nimport os\nimport glob\n\ndef get_prompt_for_property(property_name):\n    \"\"\"Get evaluation prompt for a specific property.\"\"\"\n    prompts = {\n        'top5_desk': \"In your capacity as an editorial board/reviewer for one of the most prestigious and highly selective top-5 economics journals (such as Econometrica, Journal of Political Economy, or The Quarterly Journal of Economics), please determine whether you would allow this submission to proceed past the desk review stage (0 = you would definitely reject at desk, 10 = you would definitely advance to peer review).\",\n        'top5_accept': \"In your capacity as a reviewer for one of the most prestigious and highly selective top-5 economics journals (such as Econometrica, Journal of Political Economy, or The Quarterly Journal of Economics), please determine whether you would recommend this submission for publication (0 = you would definitely reject, 10 = you would definitely recommend for publication).\",\n        'top5_accept_rating': \"In your capacity as a reviewer for one of the most prestigious and highly selective top-5 economics journals (such as Econometrica, Journal of Political Economy, or The Quarterly Journal of Economics), please determine whether you would recommend this submission for publication using the following 7-point scale: 1 = Definite Reject: Fatal flaws in theory/methodology, insufficient contribution, or serious validity concerns that make the paper unsuitable for the journal, 2 = Reject with Option to Resubmit: Significant issues with theory, methodology, or contribution, but potentially salvageable with major revisions and fresh review, 3 = Major Revision: Substantial changes needed to theory, empirics, or exposition, but the core contribution is promising enough to warrant another round, 4 = Minor Revision: Generally strong paper with few small changes needed in exposition, robustness checks, or literature discussion, 5 = Very Minor Revision: Excellent contribution needing only technical corrections or minor clarifications, 6 = Accept As Is: Exceptional contribution ready for immediate publication\",\n        'top5_accept_rating_criteria': \"In your capacity as a reviewer for one of the most prestigious and highly selective top-5 economics journals (such as Econometrica, Journal of Political Economy, or The Quarterly Journal of Economics), please determine whether you would recommend this submission for publication using the following 7-point scale: 1 = Definite Reject: Fatal flaws in theory/methodology, insufficient contribution, or serious validity concerns that make the paper unsuitable for the journal, 2 = Reject with Option to Resubmit: Significant issues with theory, methodology, or contribution, but potentially salvageable with major revisions and fresh review, 3 = Major Revision: Substantial changes needed to theory, empirics, or exposition, but the core contribution is promising enough to warrant another round, 4 = Minor Revision: Generally strong paper with few small changes needed in exposition, robustness checks, or literature discussion, 5 = Very Minor Revision: Excellent contribution needing only technical corrections or minor clarifications, 6 = Accept As Is: Exceptional contribution ready for immediate publication; Papers published in the Top 5 economics journals (American Economic Review, Quarterly Journal of Economics, Journal of Political Economy, Econometrica, and Review of Economic Studies) are often distinguished from those in other journals by several key factors: 1. Depth of Contribution Originality and Innovation: Top 5 papers typically address questions of broad, foundational importance or propose groundbreaking methodologies. They often set new standards in the field or open new research avenues. Generalisability: Findings are relevant to a wide range of settings, not just niche contexts. Big Questions: These papers tackle issues with substantial implications for policy, theory, or practice. 2. Methodological Rigour High Standards of Empirical Methods: Empirical papers in Top 5 journals employ state-of-the-art econometric techniques and robust identification strategies (e.g., natural experiments, randomised controlled trials, structural modelling). Theoretical Sophistication: Theoretical contributions are mathematically rigorous and provide deep insights, often with broad applicability. Thorough Robustness Checks: Authors typically provide extensive sensitivity analyses to demonstrate the robustness of their results. 3. Writing and Presentation Quality Clarity and Structure: The narrative is compelling and accessible, even to non-specialists in the subfield, while maintaining academic precision. Polished Presentation: Papers are meticulously written, with clear figures, tables, and appendices. The results are easy to interpret and visually intuitive. Tight Argumentation: Papers avoid unnecessary digressions, focusing directly on the key question and results. 4. Data Quality Novelty of Data: Top 5 papers often leverage unique or hard-to-access datasets that enable the study of questions previously out of reach. Rigorous Cleaning and Documentation: The data handling and analysis process is highly transparent, with all steps carefully documented. 5. Relevance and Impact Policy Relevance: Many Top 5 papers have clear implications for public policy or major economic debates, making their findings influential beyond academia. Cross-Disciplinary Interest: These papers often resonate with researchers in related disciplines, such as political science, sociology, or psychology, enhancing their visibility and citation potential. Citations: Papers in Top 5 journals often become highly cited due to their broad applicability and significance. 6. Extensive Peer Review and Revisions Stringent Referee Process: Top 5 journals have rigorous review processes, often involving multiple rounds of detailed feedback and revisions. High Rejection Rates: Acceptance rates are extremely low (e.g., ~5%), ensuring only the most impactful papers are published. 7. Network Effects and Prestige Author Reputation: Papers by well-known authors or prestigious institutions are more likely to receive attention and scrutiny during the review process. Citations of Existing Literature: Top 5 papers typically build upon or challenge widely recognised works, further cementing their place in prominent scholarly conversations. Comparison with Other Journals Scope and Niche: Non-Top 5 journals may focus on narrower questions or less generalisable findings, which, while still valuable, may not have the same broad impact. Data Availability: Some journals may accept papers using less novel or standard datasets, provided the analysis is sound. Methodological Simplicity: Papers in lower-ranked journals may employ standard or less sophisticated methodologies, especially in empirical studies. Less Competitive Review Process: Non-Top 5 journals generally have higher acceptance rates and shorter review timelines, making them accessible to a broader range of researchers.\",\n        'grant': \"As a reviewer for a major research funding organization, please evaluate whether this research proposal would be competitive for major funding (0 = definitely not fundable, 10 = definitely fundable at the highest award level).\",\n        'top_conference': \"As a program committee member for prestigious economics conferences, please evaluate whether this work would be accepted for presentation (0 = definitely reject, 10 = definitely accept for prominent session).\",\n        'citation_impact': \"Based on the novelty, methodology, and potential influence of this research, please project the actual number of citations this paper will receive in the next 10 years (output should be a specific predicted citation count)\",\n        'research_award': \"As a committee member for major research awards, please evaluate whether this work could be competitive for prestigious recognition (0 = definitely not award-worthy, 10 = definitely award-worthy).\",\n        'nobel_potential': \"As a member of the Nobel Prize Committee for Economic Sciences at the Royal Swedish Academy of Sciences, please provide a realistic evaluation of whether this research publication could contribute to winning the Nobel Prize in Economics (0 = Shows no indication of Nobel Prize potential,10 = Shows definitive Nobel Prize potential)\",\n        'tenure_eval': \"As a senior member of a research university's tenure and promotion committee, please evaluate whether this research portfolio would support a strong case for tenure, considering both the quantity and quality of contributions (0 = definitely deny tenure, 10 = exceptionally strong case for tenure).\"\n        }\n    return prompts.get(property_name)\n\ndef read_full_paper(paper_id):\n    \"\"\"Read the full paper text from file.\"\"\"\n    try:\n        filepath = f\"/work/input/paper_text/{paper_id:03d}.txt\"\n        with open(filepath, 'r') as file:\n            return file.read()\n    except Exception as e:\n        print(f\"Error reading paper {paper_id}: {e}\")\n        return \"\"\n\ndef evaluate_submission(submission_text, paper_id, include_full_text, client, property_name):\n    \"\"\"Evaluate a single submission for a specific property.\"\"\"\n    prompt = get_prompt_for_property(property_name)\n    \n    full_content = submission_text\n    if include_full_text:\n        full_paper = read_full_paper(paper_id)\n        if full_paper:\n            full_content += f\"\\nFull paper: {full_paper}\"\n    \n    try:\n        is_published_check = property_name == 'published'\n        \n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": prompt},\n                {\"role\": \"user\", \"content\": full_content}\n            ],\n            response_format={\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"name\": \"submission_evaluation\",\n                    \"strict\": True,\n                    \"schema\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"rating\": {\n                                \"type\": \"number\",\n                                \"description\": f\"Rating for {property_name}\"\n                            }\n                        },\n                        \"required\": [\"rating\"],\n                        \"additionalProperties\": False\n                    }\n                }\n            },\n            temperature=1,\n            max_tokens=2024,\n            top_p=1\n        )\n        \n        result = json.loads(response.choices[0].message.content)\n        print (result.get('rating'), response.choices[0].message.content)\n        print ('----------------------------------------------------------')\n        return result.get('rating'), response.choices[0].message.content\n        \n    except Exception as e:\n        print(f\"\\nError evaluating submission: {e}\")\n        return None, \"Error\"\n\ndef get_last_processed_index(output_dir, property_name):\n    \"\"\"Get the last processed index from temporary files.\"\"\"\n    temp_files = glob.glob(f\"{output_dir}/{property_name}/{property_name}_temp_*.csv\")\n    if not temp_files:\n        return -1\n    \n    latest_temp = max(temp_files, key=os.path.getctime)\n    try:\n        temp_df = pd.read_csv(latest_temp)\n        # Find the last row where we have a complete evaluation for this property\n        eval_columns = [col for col in temp_df.columns if col.startswith(f\"{property_name}_\") and col != f\"{property_name}_mean\"]\n        last_completed = temp_df[eval_columns].notna().all(axis=1)\n        if not last_completed.any():\n            return -1\n        return temp_df[last_completed].index[-1]\n    except Exception as e:\n        print(f\"Error reading temp file: {e}\")\n        return -1\n\ndef process_submissions(input_file, property_name, base_output_dir=\"/work/output\", include_full_text=False, evaluations_per_property=3):\n    \"\"\"Process submissions for a single property with continuation capability.\n    \n    Args:\n        input_file (str): Path to the input CSV file\n        property_name (str): Name of the property to evaluate\n        base_output_dir (str): Base directory for all output files\n        include_full_text (bool): Whether to include full paper text\n        evaluations_per_property (int): Number of evaluations to perform per property\n    \"\"\"\n    \n    # Create output directories\n    property_output_dir = f\"{base_output_dir}/{property_name}\"\n    os.makedirs(property_output_dir, exist_ok=True)\n    \n    # Define main output file path\n    main_output_file = f\"{base_output_dir}/output_{property_name}.csv\"\n    \n    # Check if main output file exists and load it instead of input file if it does\n    if os.path.exists(main_output_file):\n        print(f\"Loading existing progress from {main_output_file}\")\n        df = pd.read_csv(main_output_file)\n    else:\n        print(f\"Loading from input file {input_file}\")\n        df = pd.read_csv(input_file)\n    \n    # Initialize OpenAI client\n    client = OpenAI()\n    \n    # Check for previous progress and load existing data\n    temp_files = glob.glob(f\"{property_output_dir}/{property_name}_temp_*.csv\")\n    if temp_files:\n        latest_temp = max(temp_files, key=os.path.getctime)\n        print(f\"Found previous run data in {latest_temp}, attempting to load...\")\n        try:\n            temp_df = pd.read_csv(latest_temp)\n            # Get evaluation columns from the temporary file\n            eval_columns = [f\"{property_name}_{i+1}\" for i in range(evaluations_per_property)]\n            existing_eval_columns = [col for col in eval_columns if col in temp_df.columns]\n            \n            # Copy existing evaluations to the current dataframe\n            for col in existing_eval_columns:\n                df[col] = temp_df[col]\n            \n            # Find the last fully evaluated row\n            last_processed_idx = get_last_processed_index(base_output_dir, property_name)\n            print(f\"Resuming from index {last_processed_idx + 1}\")\n        except Exception as e:\n            print(f\"Error loading previous data: {e}\")\n            last_processed_idx = -1\n    else:\n        last_processed_idx = -1\n        \n    start_idx = last_processed_idx + 1 if last_processed_idx >= 0 else 0\n    \n    # Initialize any missing evaluation columns\n    eval_columns = [f\"{property_name}_{i+1}\" for i in range(evaluations_per_property)]\n    for col in eval_columns:\n        if col not in df.columns:\n            df[col] = None\n    \n    temp_file_counter = max([int(f.split('_')[-1].split('.')[0]) for f in glob.glob(f\"{property_output_dir}/{property_name}_temp_*.csv\")] + [-1]) + 1\n    \n    # Process submissions\n    total_tasks = (len(df) - start_idx) * evaluations_per_property\n    current_task = 0\n    \n    # Convert start_idx to int to ensure proper indexing\n    start_idx = int(start_idx)\n    \n    for idx in range(start_idx, len(df)):\n        row = df.iloc[idx]\n        \n        for n in range(evaluations_per_property):\n            current_task += 1\n            column_name = f\"{property_name}_{n+1}\"\n            \n            # Skip if already processed\n            if pd.notna(df.at[idx, column_name]):\n                continue\n            \n            score, model_response = evaluate_submission(\n                row['Submission'],\n                row['Paper_id'],\n                include_full_text,\n                client,\n                property_name\n            )\n            \n            df.at[idx, column_name] = score\n            \n            # Print progress in one line\n            progress = (current_task / total_tasks) * 100\n            print(f\"\\rProgress: {progress:.1f}% | Submission_id: {row['Submission_id']} | Paper_id: {row['Paper_id']} | Score: {score}\", end='')\n            \n            # Save temporary file and update main output every 10 predictions\n            if current_task % 10 == 0:\n                try:\n                    # Calculate mean for this property\n                    eval_columns = [f\"{property_name}_{i+1}\" for i in range(evaluations_per_property)]\n                    # Convert columns to numeric, forcing errors to NaN\n                    for col in eval_columns:\n                        df[col] = pd.to_numeric(df[col], errors='coerce')\n                    # Calculate mean, handling NaN values\n                    df[f\"{property_name}_mean\"] = df[eval_columns].astype(float).mean(axis=1).round(2)\n                    \n                    # Ensure directory exists again (in case it was deleted)\n                    os.makedirs(property_output_dir, exist_ok=True)\n                    \n                    # Save temporary file\n                    temp_filename = f\"{property_output_dir}/{property_name}_temp_{temp_file_counter}.csv\"\n                    print(f\"\\nAttempting to save temp file to: {temp_filename}\")\n                    df.to_csv(temp_filename, index=False)\n                    \n                    # Update main output file\n                    main_output_file = f\"{base_output_dir}/output_{property_name}.csv\"\n                    print(f\"Attempting to save main output to: {main_output_file}\")\n                    df.to_csv(main_output_file, index=False)\n                    \n                    print(f\"Successfully saved files\")\n                    temp_file_counter += 1\n                except Exception as e:\n                    print(f\"\\nError saving files: {e}\")\n                    print(f\"Current directory structure:\")\n                    print(f\"Base output dir exists: {os.path.exists(base_output_dir)}\")\n                    print(f\"Property output dir exists: {os.path.exists(property_output_dir)}\")\n                    print(f\"Current working directory: {os.getcwd()}\")\n                    print(\"\\nColumn types:\")\n                    for col in eval_columns:\n                        print(f\"{col}: {df[col].dtype}\")\n    \n    # Calculate mean for property\n    df[f\"{property_name}_mean\"] = df[eval_columns].mean(axis=1).round(2)\n    \n    # Save final results\n    final_output_file = f\"{base_output_dir}/output_{property_name}.csv\"\n    final_output_file_backup = f\"{base_output_dir}/output_{property_name}_final.csv\"\n    df.to_csv(final_output_file, index=False)\n    df.to_csv(final_output_file_backup, index=False)\n    \n    print(f\"\\n\\nFinal results saved to {final_output_file} and {final_output_file_backup}\")\n    return df\n\n# Example usage:\n# process_submissions(\n#     input_file='input.csv',\n#     property_name='top5_accept_rating',\n#     base_output_dir='/work/output',  # Explicitly set output directory\n#     include_full_text=False,\n#     evaluations_per_property=3\n# )","outputs":[],"outputs_reference":null,"execution_count":1,"block_group":"99b3beb1026546738e39e3c2fa70ddb2","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"fb2d41ae","execution_start":1737364448593,"execution_millis":222141,"execution_context_id":"9db1e84b-05d6-44b8-9ca3-cf28f607da42","cell_id":"d7581515dea8426fab087e4257ccc505","deepnote_cell_type":"code"},"source":"process_submissions(\n    input_file='/work/process/full_30_submission.csv',\n    property_name='nobel_potential',\n    base_output_dir='/work/output',  # Explicitly set output directory\n    include_full_text=True,\n    evaluations_per_property=3\n)","outputs":[{"name":"stdout","text":"Loading existing progress from /work/output/output_nobel_potential.csv\n\nKeyboardInterrupt\n\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/f678c4c6-19ef-47be-804d-cd6d2b90ab43","execution_count":2,"block_group":"d7b79ccdc1424f94a7f51df61996e5b3","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a25c250f-64bb-477e-a263-2c8cc56f7dca' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"92ac4a850ab94154917e0f23b3ea46a4"}}