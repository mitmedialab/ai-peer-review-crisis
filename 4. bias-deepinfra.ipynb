{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import dotenv_values\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import functions.prompts as prompts\n",
        "\n",
        "config = dotenv_values(\".env\")\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=config['DEEPINFRA_TOKEN'],\n",
        "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"paper.csv\")\n",
        "df['rank'] = df['id'].apply(lambda x: x.split(\"_\")[0])\n",
        "df = df.groupby('rank').sample(n=3, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_body(text, model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"):\n",
        "    return {\n",
        "            \"model\": model,\n",
        "            \"messages\":[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": f\"{prompts.top5()}\\nPlease respond in valid JSON format that matches this schema: {str(prompts.Top5Model.model_json_schema())}. **IMPORTANT**: ONLY RESPOND WITH A JSON OBJECT CONTAINING SCORES ACCORDING TO THE ABOVE SCHEMA. THE RESPONSE MUST END WITH A CURLY BRACKET. DO NOT ADD ANALYSIS OR EXPLANATION.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": text\n",
        "                }, \n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": \"{\"\n",
        "                }],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def llm_paper(client, i, df, add):\n",
        "    text = \"\"\n",
        "    f = open(f\"output/{df.iloc[i]['id']}.txt\", \"r\")\n",
        "    text = f.read()\n",
        "    f.close()\n",
        "\n",
        "    paper = f\"PAPER TITLE: {df.iloc[i]['name']}\\n\\n{add}\\n\\nPAPER TEXT: {text}\"\n",
        "    \n",
        "    model = \"google/gemma-3-27b-it\"\n",
        "    # model = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
        "    \n",
        "    scores = client.chat.completions.create(**gen_body(paper, model=model))\n",
        "    \n",
        "    return {\n",
        "        \"scores\": [scores],\n",
        "    }\n",
        "    \n",
        "def parse_r(r, id, typ):\n",
        "    validateModel = prompts.Top5Model if typ == \"top5\" else prompts.AnalysisModel\n",
        "    try:\n",
        "        text = r.choices[0].message.content\n",
        "        if(text.startswith(\"```json\")):\n",
        "            text = text.split(\"```json\")[1].split(\"}\")[0].replace(\"'\", '\"') + \"}\"\n",
        "        else:\n",
        "            text = \"{\" + text.split(\"}\")[0].replace(\"'\", '\"') + \"}\"\n",
        "        if(text.startswith(\"{{\")):\n",
        "            text = text[1:]\n",
        "        return {\n",
        "            \"id\": id,\n",
        "            \"scores\": validateModel.model_validate(json.loads(text)).model_dump()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error {e} - {\"{\" + r.choices[0].message.content}\")\n",
        "        return {\n",
        "            \"id\": id,\n",
        "            \"scores\": None\n",
        "        }\n",
        "\n",
        "def parse_paper(rs):\n",
        "    scores = [ parse_r(x, j, \"top5\") for j, x in enumerate(rs['scores']) ]\n",
        "    return {\n",
        "        \"scores\": scores,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"gemma\"\n",
        "\n",
        "def update_df(df, i, no, score, typ, m):\n",
        "    metrics = ['score'] if typ == \"top5\" else ['originality', 'rigor', 'scope', 'impact', 'written_by_ai']\n",
        "    validateModel = prompts.Top5Model if typ == \"top5\" else prompts.AnalysisModel\n",
        "\n",
        "    for _, metric in enumerate(metrics):\n",
        "        column_name = f\"{model_name}-{m}-{metric}-{int(no)+1}\"\n",
        "        \n",
        "        if column_name not in df.columns:\n",
        "            df[column_name] = None\n",
        "\n",
        "        try:\n",
        "            o = validateModel.model_validate(score)\n",
        "            df.loc[i, column_name] = o.__dict__[metric]\n",
        "        except:\n",
        "            print(f\"ERROR | Can't update the model in in {column_name}, skipping...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "institutions = [\n",
        "       \"Massachusetts Institute of Technology\",\n",
        "       \"Harvard University\",\n",
        "       \"London School of Economics and Political Science\",\n",
        "       \"University of Cape Town\",\n",
        "       \"Nanyang Technological University\",\n",
        "       \"Chulalongkorn University\",\n",
        "   ]\n",
        "\n",
        "top_names = [\n",
        "       \"Andrei Shleifer\", \"Daron Acemoglu\", \"James J. Heckman\",\n",
        "       \"Joseph E. Stiglitz\", \"John List\", \"Carmen M. Reinhart\",\n",
        "       \"Janet Currie\", \"Esther Duflo\", \"Asli Demirguc-Kunt\",\n",
        "       \"Marianne Bertrand\"\n",
        "]\n",
        "\n",
        "\n",
        "random_names = [\n",
        "       \"Bruce S. Green\", \"Alejandro L. James\", \"Billie J. Abels\",\n",
        "       \"Paul A. Jenkins\", \"Gary L. Bodie\", \"Gail J. Doan\",\n",
        "       \"Shirley S. Hodgkins\", \"Pattie K. Reinhardt\",\n",
        "       \"Tara R. Weber\", \"Tabitha J. Cox\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t_names = [\"top\", \"ran\"]\n",
        "\n",
        "import concurrent.futures\n",
        "\n",
        "def evaluate_institution(client, i, df, ind, ins):\n",
        "    print(f\"Evaluating AFFILIATION {i} -> {df.iloc[i]['name']} for {ins}\")\n",
        "    x = llm_paper(client, i, df, f\"AFFILIATION: {ins}\")\n",
        "    y = parse_paper(x)\n",
        "\n",
        "    for j, s in enumerate(y['scores']):\n",
        "        update_df(df, i, j, s['scores'], \"top5\", f\"ins{ind}\")\n",
        "    \n",
        "    return ind, ins\n",
        "\n",
        "def evaluate_author(client, i, df, name_type_ind, t_name, ind2, name):\n",
        "    print(f\"Evaluating {t_name} {i} -> {df.iloc[i]['name']} for {name}\")\n",
        "    x = llm_paper(client, i, df, f\"AUTHOR: {name}\")\n",
        "    y = parse_paper(x)\n",
        "\n",
        "    for j, s in enumerate(y['scores']):\n",
        "        update_df(df, i, j, s['scores'], \"top5\", f\"{t_name}{ind2}\")\n",
        "    \n",
        "    return name_type_ind, ind2, name\n",
        "\n",
        "def evaluate_paper(client, i, df):\n",
        "    print(f\"Evaluating {i} -> {df.iloc[i]['name']}\")\n",
        "    \n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
        "        institution_futures = []\n",
        "        for ind, ins in enumerate(institutions):\n",
        "            future = executor.submit(evaluate_institution, client, i, df, ind, ins)\n",
        "            institution_futures.append(future)\n",
        "        \n",
        "        author_futures = []\n",
        "        for ind, names in enumerate([top_names, random_names]):\n",
        "            t_name = t_names[ind]\n",
        "            for ind2, name in enumerate(names):\n",
        "                future = executor.submit(evaluate_author, client, i, df, ind, t_name, ind2, name)\n",
        "                author_futures.append(future)\n",
        "        \n",
        "        for future in concurrent.futures.as_completed(institution_futures):\n",
        "            try:\n",
        "                ind, ins = future.result()\n",
        "                print(f\"Completed institution {ins} (index {ind})\")\n",
        "            except Exception as exc:\n",
        "                print(f\"Institution evaluation generated an exception: {exc}\")\n",
        "        \n",
        "        for future in concurrent.futures.as_completed(author_futures):\n",
        "            try:\n",
        "                name_type_ind, ind2, name = future.result()\n",
        "                t_name = t_names[name_type_ind]\n",
        "                print(f\"Completed author {name} ({t_name}{ind2})\")\n",
        "            except Exception as exc:\n",
        "                print(f\"Author evaluation generated an exception: {exc}\")\n",
        "    \n",
        "    print(f\"Completed all evaluations for {i} -> {df.iloc[i]['name']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
        "\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.makedirs(model_name)\n",
        "\n",
        "chunk = 50\n",
        "for i in range(0, len(df), chunk):\n",
        "    print(f\"PROCESSING CHUNK {(i // chunk) + 1} of {math.ceil(len(df) / chunk)}\")\n",
        "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
        "        results = list(executor.map(\n",
        "            evaluate_paper, \n",
        "            [client] * chunk,\n",
        "            [j for j in range(i, min(i+chunk, len(df)))],\n",
        "            [df] * len(df[\"file\"]),\n",
        "        ))\n",
        "        \n",
        "    df.to_csv(f\"{model_name}/{(i // chunk) + 1}.csv\", index=False)\n",
        "    \n",
        "df.to_csv(f\"results/bias_l.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(f\"results/bias_l.csv\")\n",
        "\n",
        "fallback = df[df.isna().any(axis=1)].index\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=200) as executor:\n",
        "        results = list(executor.map(\n",
        "            evaluate_paper, \n",
        "            [client] * len(fallback),\n",
        "            fallback,\n",
        "            [df] * len(fallback),\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fallback = df[df.isna().any(axis=1)].index\n",
        "fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(f\"results/bias_l.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
