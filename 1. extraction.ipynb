{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import functions.prompts as prompts\n",
    "import functions.dupes as dupes\n",
    "import functions.llm as llm\n",
    "import functions.anonymize as anonymize\n",
    "import functions.process as process\n",
    "import importlib\n",
    "\n",
    "importlib.reload(prompts)\n",
    "importlib.reload(dupes)\n",
    "importlib.reload(llm)\n",
    "importlib.reload(anonymize)\n",
    "importlib.reload(process)\n",
    "\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_paper(path, client, id, df, danger_dupe=False):\n",
    "    try:\n",
    "        print(f\"Processing {path}\")\n",
    "\n",
    "        texts = process.extract_texts_from_pdf(path)\n",
    "        metadata = process.get_paper_metadata(client, \"\\n\".join(texts))\n",
    "        print(f\"- {path} | Metadata: {metadata}\")\n",
    "\n",
    "        flagged_dupes = dupes.get_flagged_duplicates(texts, metadata, client) \\\n",
    "            if not danger_dupe \\\n",
    "            else dupes.get_flagged_duplicates_danger(texts, metadata, client)\n",
    "        formatted_texts = process.remove_segments_from_texts(texts, flagged_dupes)\n",
    "\n",
    "        print(f\"- {path} | Anonymizing Paper\")\n",
    "        anond = anonymize.anonymized_texts(formatted_texts, metadata, client, path)\n",
    "\n",
    "        print(f\"- {path} | Done Processing!\")\n",
    "            \n",
    "        result = {\n",
    "            \"metadata\": prompts.MetadataModel.model_validate_json(metadata),\n",
    "            \"original\": texts,\n",
    "            \"anond\": process.remove_page_numbers(anond),\n",
    "            \"id\": id\n",
    "        }\n",
    "        \n",
    "        idx = df.index[df['id'] == id].tolist()[0]\n",
    "\n",
    "        updated_row = df.loc[idx].copy()\n",
    "\n",
    "        updated_row['name'] = result[\"metadata\"].title\n",
    "        updated_row['authors'] = result[\"metadata\"].authors\n",
    "        updated_row['affiliations'] = result[\"metadata\"].affiliations\n",
    "        updated_row['len-original'] = len(\" \".join(result[\"original\"]).split())\n",
    "        updated_row['len-anond'] = len(\" \".join(result[\"anond\"]).split())\n",
    "\n",
    "        df.loc[idx] = updated_row\n",
    "\n",
    "        with open(f'./output/{result[\"id\"]}.txt', 'w') as f:\n",
    "            f.write(\"\\n\".join(result[\"anond\"]))\n",
    "\n",
    "        with open(f'./output/{result[\"id\"]}-original.txt', 'w') as f:\n",
    "            f.write(\"\\n\".join(result[\"original\"]))\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"- {path} | ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_paper_names(name):\n",
    "    return {\n",
    "        \"no\": name.split(\".\")[0],\n",
    "        \"name\": \" \".join(name.split(\" \")[1:]),\n",
    "        \"folder\": name\n",
    "    }\n",
    "\n",
    "journals = [ format_paper_names(name) for name in os.listdir('./Journals') ]\n",
    "journals = [ name for name in journals if len(name[\"no\"]) > 0 and (name['no'].isnumeric() or name['no'][0] == 'P') ]\n",
    "journals = sorted(journals, key=lambda k: int(k['no']) if k['no'].isnumeric() else 1000 + int(k['no'][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['id', 'file', 'name', 'journal', 'authors', 'affiliations', 'len-original', 'len-anond'])\n",
    "\n",
    "for journal in journals:\n",
    "    papers = [{\n",
    "        \"id\": f\"{journal['no']}_{i}\",\n",
    "        \"name\": paper,\n",
    "        \"path\": f\"{journal['folder']}/{paper}\"\n",
    "    } for i, paper in enumerate(os.listdir(f'./Journals/{journal[\"folder\"]}'))]\n",
    "    \n",
    "    if(len(papers) != 10):\n",
    "        print(f\"Journal {journal['no']} {journal['name']} has {len(papers)} papers\")\n",
    "    \n",
    "    for paper in papers:\n",
    "        df = pd.concat([df, pd.DataFrame([[paper['id'], paper['path'], None, journal['name'], None, None, None, None]], columns=df.columns)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main process\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=1000) as executor:\n",
    "    results = list(executor.map(\n",
    "        anonymize_paper, \n",
    "        \"./Journals/\" + df[\"file\"],\n",
    "        [client] * len(df[\"file\"]),\n",
    "        df[\"id\"],\n",
    "        [df] * len(df[\"file\"])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"checkpoint.csv\")\n",
    "fallback = df[df['name'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback iterations: run until len(fallback) == 0 // i think manual run would be nicer compared to loop in this case\n",
    "\n",
    "fallback = df[df['name'].isnull()].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=300) as executor:\n",
    "    results = list(executor.map(\n",
    "        anonymize_paper, \n",
    "        \"./Journals/\" + fallback[\"file\"],\n",
    "        [client] * len(fallback[\"file\"]),\n",
    "        fallback[\"id\"],\n",
    "        [df] * len(fallback[\"file\"]),\n",
    "    ))\n",
    "    \n",
    "# chage danger_dupe to True for some leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['name'].isnull()].copy()\n",
    "df.to_csv(\"papers-temp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up publisher -specific artifacts\n",
    "\n",
    "papers = [file for file in os.listdir('./output') if file.endswith('.txt')]\n",
    "anond = [paper for paper in papers if '-original' not in paper]\n",
    "\n",
    "for paper in anond:\n",
    "    with open(f'./output/{paper}', 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    with open(f'./output/{paper}', 'w') as f:\n",
    "        f.write(content\n",
    "                .replace(\"JEL Classification:\", \"\")\n",
    "                .replace(\"A R T I C L E I N F O\", \"\") # Elsevier\n",
    "                .replace(\"A B S T R A C T\", \"\") # Elsevier\n",
    "                .replace('O R I G I N A L A R T I C L E', '') # Wiley\n",
    "                .replace('K E Y W O R D S', '') # Wiley\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
